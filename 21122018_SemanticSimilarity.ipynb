{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlHqSdgSEwPE"
      },
      "source": [
        "# Universal Sentence Encoder-Lite\n",
        "\n",
        "> Drashty Ranpara\n",
        "\n",
        "> 21122018\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzAKFdbUmyP"
      },
      "source": [
        "**Basic Semantic Search Using Sentence Embeddings** \\\\\n",
        "Sematically Similarity Using fine tuned pre-trained BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedDvAy3M4jd"
      },
      "source": [
        "**Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks** \\\\\n",
        "[GitHub](https://github.com/UKPLab/sentence-transformers) \\\\\n",
        "[Paper](https://arxiv.org/pdf/1908.10084.pdf) \\\\\n",
        "Sentence-BERT (SBERT), a modification of the pretrained\n",
        "BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the\n",
        "effort for finding the most similar pair from 65\n",
        "hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xHlXt5zMveR"
      },
      "source": [
        "**Installation and Enviornment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9GjZZIMwPB"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE331XNHMnha"
      },
      "source": [
        "**Sentences Embedding with a Pretrained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulMBhMRxMjNC"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twn5-N6YNtFc"
      },
      "source": [
        "\n",
        "The models were first fine-tuned on the AllNLI dataset, then on train set of STS benchmark. They are specifically well suited for semantic textual similarity. For more details, see: sts-models.md.\n",
        "\n",
        "**Pretrained Models** \\\\\n",
        "bert-base-nli-stsb-mean-tokens: Performance: STSbenchmark: 85.14 \\\\\n",
        "bert-large-nli-stsb-mean-tokens: Performance: STSbenchmark: 85.29 \\\\\n",
        "roberta-base-nli-stsb-mean-tokens: Performance: STSbenchmark: 85.44 \\\\\n",
        "roberta-large-nli-stsb-mean-tokens: Performance: STSbenchmark: 86.39 \\\\\n",
        "distilbert-base-nli-stsb-mean-tokens: Performance: STSbenchmark: 84.38 \\\\\n",
        "**Source: \"*Github Repository*\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx6BoYDXNfSW"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdwTSHbNOH2w"
      },
      "source": [
        "sentences = ['Absence of sanity', \n",
        "             'Lack of saneness',\n",
        "             'A man is eating food.',\n",
        "             'A man is eating a piece of bread.',\n",
        "             'The girl is carrying a baby.',\n",
        "             'A man is riding a horse.',\n",
        "             'A woman is playing violin.',\n",
        "             'Two men pushed carts through the woods.',\n",
        "             'A man is riding a white horse on an enclosed ground.',\n",
        "             'A monkey is playing drums.',\n",
        "             'A cheetah is running behind its prey.']\n",
        "sentence_embeddings_base = model.encode(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZhHjplkOcQM"
      },
      "source": [
        "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTaHpfAMPBor"
      },
      "source": [
        "Each sentence embedding have a shape of [768 x 1]. \\\\\n",
        "Let's play some more with other pre-trained model, for e.g \"roberta-large-nli-mean-tokens\". Let's have a look.  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSHATDVO5y8"
      },
      "source": [
        "model_roberta = SentenceTransformer('roberta-large-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAlMx99VPtuS"
      },
      "source": [
        "sentences = ['Absence of sanity', \n",
        "             'Lack of saneness',\n",
        "             'A man is eating food.',\n",
        "             'A man is eating a piece of bread.',\n",
        "             'The girl is carrying a baby.',\n",
        "             'A man is riding a horse.',\n",
        "             'A woman is playing violin.',\n",
        "             'Two men pushed carts through the woods.',\n",
        "             'A man is riding a white horse on an enclosed ground.',\n",
        "             'A monkey is playing drums.',\n",
        "             'A cheetah is running behind its prey.']\n",
        "sentence_embeddings = model_roberta.encode(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IbfI1VbQera"
      },
      "source": [
        "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoHQPs5RQk70"
      },
      "source": [
        "**Semantic Search** Using Sentence-BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bntpkqIvQf0S"
      },
      "source": [
        "query = 'Nobody has sane thoughts'  #  A query sentence uses for searching semantic similarity score.\n",
        "queries = [query]\n",
        "query_embeddings = model.encode(queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8ZaHU0OR984"
      },
      "source": [
        "!pip install scipy\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBrN6ROReys"
      },
      "source": [
        "print(\"Semantic Search Results\")\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings_base, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in results[0:number_top_matches]:\n",
        "        print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0HuiScHQ3OK"
      },
      "source": [
        "This Colab illustrates how to use the Universal Sentence Encoder-Lite for sentence similarity task. This module is very similar to [Universal Sentence Encoder](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/2) with the only difference that you need to run [SentencePiece](https://github.com/google/sentencepiece) processing on your input sentences.\n",
        "\n",
        "The Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqCB2pyK-WSU"
      },
      "source": [
        "# Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWeEjoO5M0Cx"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5_potQBMzcU"
      },
      "outputs": [],
      "source": [
        "# Install seaborn for pretty visualizations\n",
        "!pip3 install --quiet seaborn\n",
        "# Install SentencePiece package\n",
        "# SentencePiece package is needed for Universal Sentence Encoder Lite. We'll\n",
        "# use it for all the text processing and sentence feature ID lookup.\n",
        "!pip3 install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMTa6V4a-cmf"
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPXYQDBiFJHd"
      },
      "source": [
        "## Load the module from TF-Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEWUT-lmAkxM"
      },
      "outputs": [],
      "source": [
        "module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5277Z-9qARYF"
      },
      "outputs": [],
      "source": [
        "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
        "encodings = module(\n",
        "    inputs=dict(\n",
        "        values=input_placeholder.values,\n",
        "        indices=input_placeholder.indices,\n",
        "        dense_shape=input_placeholder.dense_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yydbhuba_nek"
      },
      "source": [
        "## Load SentencePiece model from the TF-Hub Module\n",
        "The SentencePiece model is conveniently stored inside the module's assets. It has to be loaded in order to initialize the processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CyUjKzE_tcJ"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "with tf.io.gfile.GFile(spm_path, mode=\"rb\") as f:\n",
        "  sp.LoadFromSerializedProto(f.read())\n",
        "print(\"SentencePiece model loaded at {}.\".format(spm_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y5kkN-l-5QV"
      },
      "outputs": [],
      "source": [
        "def process_to_IDs_in_sparse_format(sp, sentences):\n",
        "  # An utility method that processes sentences with the sentence piece processor\n",
        "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
        "  # (values, indices, dense_shape)\n",
        "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
        "  max_len = max(len(x) for x in ids)\n",
        "  dense_shape=(len(ids), max_len)\n",
        "  values=[item for sublist in ids for item in sublist]\n",
        "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
        "  return (values, indices, dense_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpHEWrPAdxR"
      },
      "source": [
        "### Test the module with a few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSkjuGYoCBfU"
      },
      "outputs": [],
      "source": [
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
        "\n",
        "# Reduce logging output.\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    print(\"Message: {}\".format(messages[i]))\n",
        "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46jrIgHyFDz9"
      },
      "source": [
        "# Semantic Textual Similarity (STS) task example\n",
        "\n",
        "The embeddings produced by the Universal Sentence Encoder are approximately normalized. The semantic similarity of two sentences can be trivially computed as the inner product of the encodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIQudHgWBGSk"
      },
      "outputs": [],
      "source": [
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "\n",
        "def run_and_plot(session, input_placeholder, messages):\n",
        "  values, indices, dense_shape = process_to_IDs_in_sparse_format(sp,messages)\n",
        "\n",
        "  message_embeddings = session.run(\n",
        "      encodings,\n",
        "      feed_dict={input_placeholder.values: values,\n",
        "                input_placeholder.indices: indices,\n",
        "                input_placeholder.dense_shape: dense_shape})\n",
        "  \n",
        "  plot_similarity(messages, message_embeddings, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDqttNcE0Bx"
      },
      "source": [
        "## Similarity visualized\n",
        "Here we show the similarity in a heat map. The final graph is a 9x9 matrix where each entry `[i, j]` is colored based on the inner product of the encodings for sentence `i` and `j`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSCW5QIBKVe"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  run_and_plot(session, input_placeholder, messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZ4sRBYBnL8"
      },
      "source": [
        "## Evaluation: STS (Semantic Textual Similarity) Benchmark\n",
        "\n",
        "The [**STS Benchmark**](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is then used to evaluate the quality of the machine similarity scores against human judgements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNMVfSelBsHW"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zAWVzBMBptq"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "\n",
        "\n",
        "def load_sts_dataset(filename):\n",
        "  # Loads a subset of the STS dataset into a DataFrame. In particular both\n",
        "  # sentences and their human rated similarity score.\n",
        "  sent_pairs = []\n",
        "  with tf.gfile.GFile(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      ts = line.strip().split(\"\\t\")\n",
        "      # (sent_1, sent_2, similarity_score)\n",
        "      sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
        "  return pandas.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "def download_and_load_sts_data():\n",
        "  sts_dataset = tf.keras.utils.get_file(\n",
        "      fname=\"Stsbenchmark.tar.gz\",\n",
        "      origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "      extract=True)\n",
        "\n",
        "  sts_dev = load_sts_dataset(\n",
        "      os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
        "  sts_test = load_sts_dataset(\n",
        "      os.path.join(\n",
        "          os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
        "\n",
        "  return sts_dev, sts_test\n",
        "\n",
        "\n",
        "sts_dev, sts_test = download_and_load_sts_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8lEawD6B4Fr"
      },
      "source": [
        "### Build evaluation graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etiZUkP-B6bR"
      },
      "outputs": [],
      "source": [
        "sts_input1 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "sts_input2 = tf.sparse_placeholder(tf.int64, shape=(None, None))\n",
        "\n",
        "# For evaluation we use exactly normalized rather than\n",
        "# approximately normalized.\n",
        "sts_encode1 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input1.values,\n",
        "                    indices=sts_input1.indices,\n",
        "                    dense_shape=sts_input1.dense_shape)),\n",
        "    axis=1)\n",
        "sts_encode2 = tf.nn.l2_normalize(\n",
        "    module(\n",
        "        inputs=dict(values=sts_input2.values,\n",
        "                    indices=sts_input2.indices,\n",
        "                    dense_shape=sts_input2.dense_shape)),\n",
        "    axis=1)\n",
        "\n",
        "sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Q34ssLB-rw"
      },
      "source": [
        "### Evaluate sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vRFEFPJPyeF"
      },
      "outputs": [],
      "source": [
        "#@title Choose dataset for benchmark\n",
        "dataset = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "\n",
        "values1, indices1, dense_shape1 = process_to_IDs_in_sparse_format(sp, dataset['sent_1'].tolist())\n",
        "values2, indices2, dense_shape2 = process_to_IDs_in_sparse_format(sp, dataset['sent_2'].tolist())\n",
        "similarity_scores = dataset['sim'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJ2DI85CBDh"
      },
      "outputs": [],
      "source": [
        "def run_sts_benchmark(session):\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  scores = session.run(\n",
        "      sim_scores,\n",
        "      feed_dict={\n",
        "          sts_input1.values: values1,\n",
        "          sts_input1.indices:  indices1,\n",
        "          sts_input1.dense_shape:  dense_shape1,\n",
        "          sts_input2.values:  values2,\n",
        "          sts_input2.indices:  indices2,\n",
        "          sts_input2.dense_shape:  dense_shape2,\n",
        "      })\n",
        "  return scores\n",
        "\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  scores = run_sts_benchmark(session)\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, similarity_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Textual Similarity using sentence_transformers**\n",
        "---\n",
        "Once you have sentence embeddings computed, you usually want to compare them to each other. Here, I show you how you can compute the cosine similarity between embeddings, for example, to measure the semantic similarity of two texts."
      ],
      "metadata": {
        "id": "2xBR72XPCR3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "p2Jxf2HkCGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "id": "rYLofFCPCE6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the convert_to_tensor=True parameter to the encode function. This will return a pytorch tensor containing our embeddings. We can then call `util.cos_sim(A, B)` which computes the cosine similarity between all vectors in A and all vectors in B.\n",
        "\n",
        "It returns in the above example a 3x3 matrix with the respective cosine similarity scores for all possible pairs between embeddings1 and embeddings2.\n",
        "\n",
        "You can use this function also to find out the pairs with the highest cosine similarity scores:"
      ],
      "metadata": {
        "id": "Q98Qk6GxCfrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Single list of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "#Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
      ],
      "metadata": {
        "id": "H5mYXK6GCiKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, in the above approach we use a **brute-force** approach to find the highest scoring pairs, which has a quadratic complexity. For long lists of sentences, this might be infeasible."
      ],
      "metadata": {
        "id": "7qZRnHU9CojM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IJhWonqQN7u0"
      ],
      "name": "Semantic Similarity with TF-Hub Universal Encoder Lite",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}